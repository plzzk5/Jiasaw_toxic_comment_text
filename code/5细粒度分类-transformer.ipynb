{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入需要的模块\n",
    "import tensorflow as tf # tensorflow模块，深度学习框架\n",
    "import keras # keras模块，深度学习框架\n",
    "import re # 正则表达式\n",
    "import numpy as np # numpy数组\n",
    "import pandas as pd # pandas表格\n",
    "import matplotlib.pyplot as plt # matplotlib 数据可视化\n",
    "from sklearn.model_selection import train_test_split # 训练集、验证集、测试集的划分\n",
    "from tensorflow.keras.models import load_model # 用于加载模型\n",
    "from sklearn.utils import class_weight # 用于计算样本的权重\n",
    "from sklearn.metrics import roc_curve, auc # 用于绘画ROC曲线，计算AUC值\n",
    "from keras.preprocessing.image import ImageDataGenerator # keras模块的图片预处理模块，可用于数据增强\n",
    "from keras.optimizers import adam_v2 # Adam优化器\n",
    "from collections import Counter # 词频统计包\n",
    "import time\n",
    "from keras.callbacks import TensorBoard\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>non_toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>168ac3d396c7d588</td>\n",
       "      <td>if there is a chromosone then e=what is it?Sma...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>168bc99fa2cfd9aa</td>\n",
       "      <td>Hollywood Undead \\n\\nI have collected articles...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>168cd51c24508159</td>\n",
       "      <td>\"\\n\\n Rollback \\n\\nI've enabled rollback on yo...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>168d515e2e99f78d</td>\n",
       "      <td>Another backlog. Thanks. (Trouble?/My Work)</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>168d5a1c66f5e8bf</td>\n",
       "      <td>\" - unsigned\\n\\nWe do include it. This article...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 id                                       comment_text  toxic  \\\n",
       "0  168ac3d396c7d588  if there is a chromosone then e=what is it?Sma...      0   \n",
       "1  168bc99fa2cfd9aa  Hollywood Undead \\n\\nI have collected articles...      0   \n",
       "2  168cd51c24508159  \"\\n\\n Rollback \\n\\nI've enabled rollback on yo...      0   \n",
       "3  168d515e2e99f78d        Another backlog. Thanks. (Trouble?/My Work)      0   \n",
       "4  168d5a1c66f5e8bf  \" - unsigned\\n\\nWe do include it. This article...      0   \n",
       "\n",
       "   severe_toxic  obscene  threat  insult  identity_hate  non_toxic  \n",
       "0             0        0       0       0              0          1  \n",
       "1             0        0       0       0              0          1  \n",
       "2             0        0       0       0              0          1  \n",
       "3             0        0       0       0              0          1  \n",
       "4             0        0       0       0              0          1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "train_data = pd.read_csv('train_data.csv',encoding='ISO-8859-1')\n",
    "test_data = pd.read_csv('test_data.csv',encoding='ISO-8859-1')\n",
    "train_data['non_toxic'] = train_data['toxic'].apply(lambda x:1 if x==0 else 0)\n",
    "test_data['non_toxic'] = test_data['toxic'].apply(lambda x:1 if x==0 else 0)\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "form_dict={'severe_toxic':1,'obscene':2,'threat':3,'insult':4,'identity_hate':5,'non_toxic':0}\n",
    "\n",
    "tmp_traindata = train_data[['severe_toxic','obscene','threat','insult','identity_hate','non_toxic']]\n",
    "train_data['Form'] = tmp_traindata.apply(lambda row:form_dict[tmp_traindata.columns[row.tolist().index(max(row.tolist()))]] , axis = 1 )\n",
    "\n",
    "tmp_testdata = test_data[['severe_toxic','obscene','threat','insult','identity_hate','non_toxic']]\n",
    "test_data['Form'] = tmp_testdata.apply(lambda row:form_dict[tmp_testdata.columns[row.tolist().index(max(row.tolist()))]] , axis = 1 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    193488\n",
       "2      9212\n",
       "1      8116\n",
       "4      2106\n",
       "5       249\n",
       "3       205\n",
       "Name: Form, dtype: int64"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['Form'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词时间：4.611855506896973\n"
     ]
    }
   ],
   "source": [
    "# 处理训练集\n",
    "# 将对应的标签拿出来\n",
    "#train_labels = np.array(train_data['Form'], dtype=np.int32)\n",
    "train_labels = pd.get_dummies(train_data['Form'])\n",
    "t0 = time.time()\n",
    "# 分词处理\n",
    "train_intro_texts = []\n",
    "for intro in train_data['comment_text']:\n",
    "    intro = intro.replace(',',' ').replace('.',' ').replace('\\n',' ').replace('?',' ').replace('!',' ').replace(';',' ').replace(':',' ')\n",
    "    intro = intro.replace('(',' ').replace(')',' ').replace('[',' ').replace(']',' ').replace('{',' ').replace('}',' ')\n",
    "    intro = intro.replace('\"',' ').replace('.\"',' ').replace('?\"',' ').replace('!\"',' ').replace('-','')\n",
    "    intro = intro.lower()\n",
    "    intro = [i for i in intro.split()]\n",
    "    train_intro_texts.append(intro)\n",
    "print(\"分词时间：%s\"%(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "分词时间：0.19749951362609863\n"
     ]
    }
   ],
   "source": [
    "# 处理测试集\n",
    "# 将对应的标签拿出来\n",
    "#test_labels = np.array(test_data['Form'], dtype=np.int32)\n",
    "test_labels = pd.get_dummies(test_data['Form'])\n",
    "t0 = time.time()\n",
    "# 分词处理\n",
    "test_intro_texts = []\n",
    "for intro in test_data['comment_text']:\n",
    "    intro = intro.replace(',',' ').replace('.',' ').replace('\\n',' ').replace('?',' ').replace('!',' ').replace(';',' ').replace(':',' ')\n",
    "    intro = intro.replace('(',' ').replace(')',' ').replace('[',' ').replace(']',' ').replace('{',' ').replace('}',' ')\n",
    "    intro = intro.replace('\"',' ').replace('.\"',' ').replace('?\"',' ').replace('!\"',' ').replace('-','')\n",
    "    intro = intro.lower()\n",
    "    intro = [i for i in intro.split()]\n",
    "    test_intro_texts.append(intro)\n",
    "print(\"分词时间：%s\"%(time.time()-t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['statto', 'com', 'hello', 'yes', \"it's\", 'been', 'back', 'for', 'a', 'couple', 'of', 'weeks', 'now', 'hopefully', 'it', 'intends', 'to', 'stay', 'up', 'this', 'time', 'cheers']\n",
      "134115    1\n",
      "3236      1\n",
      "125329    1\n",
      "18471     1\n",
      "167131    1\n",
      "         ..\n",
      "206152    1\n",
      "22791     1\n",
      "6923      1\n",
      "190231    1\n",
      "80092     1\n",
      "Name: 0, Length: 170700, dtype: uint8\n",
      "170700\n",
      "170700\n"
     ]
    }
   ],
   "source": [
    "# 训练集，验证集，测试集的划分\n",
    "# text_s,text_test_s,\\\n",
    "# label_s,label_test_s = train_test_split(\n",
    "#     intro_texts,labels,\n",
    "#     test_size=1.0, \n",
    "#     stratify=labels,\n",
    "#     random_state=233)\n",
    "text_train,text_val,\\\n",
    "label_train,label_val = train_test_split(\n",
    "    train_intro_texts,train_labels,\n",
    "    test_size=0.2, \n",
    "    stratify=train_labels,\n",
    "    random_state=233)  # 训练集和验证集的划分\n",
    "text_test = test_intro_texts\n",
    "label_test = test_labels\n",
    "print(text_train[0])\n",
    "print(label_train[0])\n",
    "print(len(text_train))\n",
    "print(len(label_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "intro_lengths = [len(introduction) for introduction in text_train]#统计评论的长度\n",
    "\n",
    "SENLEN = 400\n",
    "\n",
    "# VOCAB_SIZE 的决定:训练集上该词出现的次数>=2\n",
    "train_word_list = [word for sequence in text_train for word in sequence]\n",
    "word_counter_introduction = Counter(train_word_list)#统计每个词出现的次数\n",
    "most_common_word_in_train_introduction = word_counter_introduction.most_common()#词频降序排序\n",
    "VOCAB_SIZE = len(most_common_word_in_train_introduction) + 1 # 设置一个初值，大概率结果不是这个\n",
    "for i in range(len(most_common_word_in_train_introduction)):\n",
    "    if most_common_word_in_train_introduction[i][1] <= 1:\n",
    "        VOCAB_SIZE = i + 1 #VOCAB_SIZE设为词频>=2的词的数量\n",
    "        break\n",
    "        \n",
    "# 对训练集建表\n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=VOCAB_SIZE)\n",
    "tokenizer.fit_on_texts(text_train)\n",
    "\n",
    "# 将训练集的文本数字化\n",
    "text_train = tokenizer.texts_to_sequences(text_train)\n",
    "text_train = keras.preprocessing.sequence.pad_sequences(\n",
    "    text_train, maxlen=SENLEN, padding='post')\n",
    "\n",
    "# 将验证集的文本数字化\n",
    "text_val = tokenizer.texts_to_sequences(text_val)\n",
    "text_val = keras.preprocessing.sequence.pad_sequences(\n",
    "    text_val, maxlen=SENLEN, padding='post')\n",
    "\n",
    "# 将测试集的文本数字化\n",
    "text_test = tokenizer.texts_to_sequences(text_test)\n",
    "text_test = keras.preprocessing.sequence.pad_sequences(\n",
    "    text_test, maxlen=SENLEN, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "#! -*- coding: utf-8 -*-\n",
    "#%%\n",
    "from __future__ import print_function\n",
    "from tensorflow.keras import backend as K\n",
    "from keras.layers import Layer\n",
    " \n",
    "class Position_Embedding(Layer):\n",
    " \n",
    "    def __init__(self, size=None, mode='sum', **kwargs):\n",
    "        self.size = size #必须为偶数\n",
    "        self.mode = mode\n",
    "        super(Position_Embedding, self).__init__(**kwargs)\n",
    " \n",
    "    def call(self, x):\n",
    "        if (self.size == None) or (self.mode == 'sum'):\n",
    "            self.size = int(x.shape[-1])\n",
    "        batch_size,seq_len = K.shape(x)[0],K.shape(x)[1]\n",
    "        position_j = 1. / K.pow(10000., \\\n",
    "                                 2 * K.arange(self.size / 2, dtype='float32' \\\n",
    "                               ) / self.size)\n",
    "        position_j = K.expand_dims(position_j, 0)\n",
    "        position_i = K.cumsum(K.ones_like(x[:,:,0]), 1)-1 #K.arange不支持变长，只好用这种方法生成\n",
    "        position_i = K.expand_dims(position_i, 2)\n",
    "        position_ij = K.dot(position_i, position_j)\n",
    "        position_ij = K.concatenate([K.cos(position_ij), K.sin(position_ij)], 2)\n",
    "        if self.mode == 'sum':\n",
    "            return position_ij + x\n",
    "        elif self.mode == 'concat':\n",
    "            return K.concatenate([position_ij, x], 2)\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.mode == 'sum':\n",
    "            return input_shape\n",
    "        elif self.mode == 'concat':\n",
    "            return (input_shape[0], input_shape[1], input_shape[2]+self.size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "class Attention(Layer):\n",
    " \n",
    "    def __init__(self, nb_head, size_per_head, **kwargs):\n",
    "        super(Attention, self).__init__(**kwargs)\n",
    "        self.nb_head = nb_head\n",
    "        self.size_per_head = 2*size_per_head\n",
    "        self.output_dim = nb_head*size_per_head\n",
    "        \n",
    "    def build(self, input_shape):\n",
    "        self.WQ = self.add_weight(name='WQ',\n",
    "                                  shape=(input_shape[0][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WK = self.add_weight(name='WK',\n",
    "                                  shape=(input_shape[1][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        self.WV = self.add_weight(name='WV',\n",
    "                                  shape=(input_shape[2][-1], self.output_dim),\n",
    "                                  initializer='glorot_uniform',\n",
    "                                  trainable=True)\n",
    "        super(Attention, self).build(input_shape)\n",
    " \n",
    "    def Mask(self, inputs, seq_len, mode='mul'):\n",
    "        if seq_len == None:\n",
    "            return inputs\n",
    "        else:\n",
    "            mask = K.one_hot(seq_len[:,0], K.shape(inputs)[1])\n",
    "            mask = 1 - K.cumsum(mask, 1)\n",
    "            for _ in range(len(inputs.shape)-2):\n",
    "                mask = K.expand_dims(mask, 2)\n",
    "            if mode == 'mul':\n",
    "                return inputs * mask\n",
    "            if mode == 'add':\n",
    "                return inputs - (1 - mask) * 1e12\n",
    " \n",
    "    def call(self,x):\n",
    "        #如果只传入Q_seq,K_seq,V_seq，那么就不做Mask\n",
    "        #如果同时传入Q_seq,K_seq,V_seq,Q_len,V_len，那么对多余部分做Mask\n",
    "        if len(x) == 3:\n",
    "            Q_seq,K_seq,V_seq = x\n",
    "            Q_len,V_len = None,None\n",
    "        elif len(x) == 5:\n",
    "            Q_seq,K_seq,V_seq,Q_len,V_len = x\n",
    "        #对Q、K、V做线性变换\n",
    "        Q_seq = K.dot(Q_seq, self.WQ)\n",
    "        Q_seq = K.reshape(Q_seq, (-1, K.shape(Q_seq)[1], self.nb_head, self.size_per_head))\n",
    "        Q_seq = K.permute_dimensions(Q_seq, (0,2,1,3))\n",
    "        K_seq = K.dot(K_seq, self.WK)\n",
    "        K_seq = K.reshape(K_seq, (-1, K.shape(K_seq)[1], self.nb_head, self.size_per_head))\n",
    "        K_seq = K.permute_dimensions(K_seq, (0,2,1,3))\n",
    "        V_seq = K.dot(V_seq, self.WV)\n",
    "        V_seq = K.reshape(V_seq, (-1, K.shape(V_seq)[1], self.nb_head, self.size_per_head))\n",
    "        V_seq = K.permute_dimensions(V_seq, (0,2,1,3))\n",
    "        #计算内积，然后mask，然后softmax\n",
    "        A = K.batch_dot(Q_seq, K_seq, axes=[3,3]) / self.size_per_head**0.5\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = self.Mask(A, V_len, 'add')\n",
    "        A = K.permute_dimensions(A, (0,3,2,1))\n",
    "        A = K.softmax(A)\n",
    "        #输出并mask\n",
    "        O_seq = K.batch_dot(A, V_seq, axes=[3,2])\n",
    "        O_seq = K.permute_dimensions(O_seq, (0,2,1,3))\n",
    "        O_seq = K.reshape(O_seq, (-1, K.shape(O_seq)[1], self.output_dim))\n",
    "        O_seq = self.Mask(O_seq, Q_len, 'mul')\n",
    "        return O_seq\n",
    " \n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (input_shape[0][0], input_shape[0][1], self.output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "def TransformerBlock(x,nb_head,ff_dim):\n",
    "    O_seq = Attention(nb_head)([x,x,x])\n",
    "    O_seq = keras.layers.add([O_seq,x])\n",
    "    O_seq = keras.layers.LayerNormalization(epsilon=1e-6)(O_seq)\n",
    "    x_res = O_seq\n",
    "    O_seq = keras.layers.Dense(ff_dim,activation='relu')(O_seq)\n",
    "    O_seq = keras.layers.Dense(ff_dim,activation='relu')(O_seq)\n",
    "    O_seq = keras.layers.add([O_seq,x_res])\n",
    "    O_seq = keras.layers.LayerNormalization(epsilon=1e-6)(O_seq)\n",
    "    return O_seq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 尝试随机过采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "928740\n",
      "170700\n"
     ]
    }
   ],
   "source": [
    "from imblearn.over_sampling import RandomOverSampler\n",
    "ros = RandomOverSampler(random_state=0)\n",
    "X_resampled, y_resampled = ros.fit_resample(text_train,label_train)\n",
    "print(len(X_resampled))\n",
    "#print(sorted(Counter(y_resampled).items()))\n",
    "print(len(text_train))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transformer 测试"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_14 (InputLayer)           [(None, 400)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_12 (Embedding)        (None, 400, 128)     14036608    input_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "position__embedding_12 (Positio (None, 400, 128)     0           embedding_12[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "attention_8 (Attention)         (None, 400, 128)     1           position__embedding_12[0][0]     \n",
      "                                                                 position__embedding_12[0][0]     \n",
      "                                                                 position__embedding_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "add_4 (Add)                     (None, 400, 128)     0           attention_8[0][0]                \n",
      "                                                                 position__embedding_12[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_3 (LayerNor (None, 400, 128)     256         add_4[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_10 (Dense)                (None, 400, 128)     16512       layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_11 (Dense)                (None, 400, 128)     16512       dense_10[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_5 (Add)                     (None, 400, 128)     0           dense_11[0][0]                   \n",
      "                                                                 layer_normalization_3[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_4 (LayerNor (None, 400, 128)     256         add_5[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_9 (Attention)         (None, 400, 128)     1           layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_6 (Add)                     (None, 400, 128)     0           attention_9[0][0]                \n",
      "                                                                 layer_normalization_4[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_5 (LayerNor (None, 400, 128)     256         add_6[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_12 (Dense)                (None, 400, 128)     16512       layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_13 (Dense)                (None, 400, 128)     16512       dense_12[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_7 (Add)                     (None, 400, 128)     0           dense_13[0][0]                   \n",
      "                                                                 layer_normalization_5[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_6 (LayerNor (None, 400, 128)     256         add_7[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_10 (Attention)        (None, 400, 128)     1           layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_8 (Add)                     (None, 400, 128)     0           attention_10[0][0]               \n",
      "                                                                 layer_normalization_6[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_7 (LayerNor (None, 400, 128)     256         add_8[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "dense_14 (Dense)                (None, 400, 128)     16512       layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_15 (Dense)                (None, 400, 128)     16512       dense_14[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_9 (Add)                     (None, 400, 128)     0           dense_15[0][0]                   \n",
      "                                                                 layer_normalization_7[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_8 (LayerNor (None, 400, 128)     256         add_9[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "attention_11 (Attention)        (None, 400, 128)     1           layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "add_10 (Add)                    (None, 400, 128)     0           attention_11[0][0]               \n",
      "                                                                 layer_normalization_8[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_9 (LayerNor (None, 400, 128)     256         add_10[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "dense_16 (Dense)                (None, 400, 128)     16512       layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "dense_17 (Dense)                (None, 400, 128)     16512       dense_16[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "add_11 (Add)                    (None, 400, 128)     0           dense_17[0][0]                   \n",
      "                                                                 layer_normalization_9[0][0]      \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization_10 (LayerNo (None, 400, 128)     256         add_11[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_6 (Glo (None, 128)          0           layer_normalization_10[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dropout_6 (Dropout)             (None, 128)          0           global_average_pooling1d_6[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "dense_18 (Dense)                (None, 6)            774         dropout_6[0][0]                  \n",
      "==================================================================================================\n",
      "Total params: 14,171,530\n",
      "Trainable params: 14,171,530\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/myconda/lib/python3.8/site-packages/keras/optimizer_v2/optimizer_v2.py:357: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "model : Transformer\n",
    "'''\n",
    "batch_size = 256\n",
    "EMB_SIZE = 128\n",
    "nb_head = 8\n",
    "ff_dim = 128\n",
    "from keras.layers import *\n",
    "S_inputs = Input(shape=(SENLEN,), dtype='int32') \n",
    "embeddings = Embedding(VOCAB_SIZE, EMB_SIZE)(S_inputs)\n",
    "embeddings = Position_Embedding()(embeddings) #增加Position_Embedding能轻微提高准确率\n",
    "O_seq = TransformerBlock(embeddings,nb_head,ff_dim)\n",
    "for i in range(3):\n",
    "    O_seq = TransformerBlock(O_seq,nb_head,ff_dim)\n",
    "O_seq = GlobalAveragePooling1D()(O_seq)\n",
    "O_seq = Dropout(0.5)(O_seq)\n",
    "outputs = Dense(6, activation='softmax')(O_seq)\n",
    "model = Model(inputs=S_inputs, outputs=outputs)\n",
    "# try using different optimizers and different optimizer configs\n",
    "opt = adam_v2.Adam(lr=0.0001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\n",
    "loss = 'categorical_crossentropy'\n",
    "#model.compile(loss=loss,optimizer=opt,metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\"adam\", \"categorical_crossentropy\", metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "2668/2668 [==============================] - 774s 290ms/step - loss: 0.1408 - accuracy: 0.9486 - val_loss: 0.2427 - val_accuracy: 0.9305\n",
      "Epoch 2/5\n",
      "2668/2668 [==============================] - 787s 295ms/step - loss: 0.1250 - accuracy: 0.9553 - val_loss: 0.2564 - val_accuracy: 0.9311\n",
      "Epoch 3/5\n",
      "2668/2668 [==============================] - 793s 297ms/step - loss: 0.1098 - accuracy: 0.9606 - val_loss: 0.2771 - val_accuracy: 0.9299\n",
      "Epoch 4/5\n",
      "2668/2668 [==============================] - 790s 296ms/step - loss: 0.0940 - accuracy: 0.9665 - val_loss: 0.3069 - val_accuracy: 0.9241\n",
      "Epoch 5/5\n",
      "2668/2668 [==============================] - 790s 296ms/step - loss: 0.0800 - accuracy: 0.9715 - val_loss: 0.3524 - val_accuracy: 0.9171\n"
     ]
    }
   ],
   "source": [
    "early2_stopping = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5, mode='min') # 采用早停的策略\n",
    "history = model.fit(\n",
    "    text_train,label_train, batch_size=64, epochs=5, validation_data=(text_val,label_val)\n",
    ")\n",
    "#model2.save('model2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = LabelEncoder()\n",
    "# make predictions\n",
    "pred = model.predict(text_test)\n",
    "#pred = encoder.fit_transform(pred)\n",
    "# inverse numeric variables to initial categorical labels\n",
    "pred_labels = [np.argmax(x) for x in pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[7412,  168,    9,    0,    4,    0],\n",
       "       [ 517,  339,  309,    1,   54,    2],\n",
       "       [ 130,  192,  691,    1,   34,    0],\n",
       "       [  11,    9,    1,    1,    5,    1],\n",
       "       [  84,   98,   27,    1,   50,    0],\n",
       "       [   4,    9,    4,    0,    5,    0]])"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cf = confusion_matrix(test_data['Form'],pred_labels)\n",
    "cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.908876\n",
      "Precision: 0.908556\n",
      "Recall: 0.976162\n",
      "Specificity: 0.710853\n",
      "F1-Score: 0.941147\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy: %3f\"%((TP+TN)/(TP+TN+FP+FN)))\n",
    "print(\"Precision: %3f\"%(TP/(TP+FP)))\n",
    "print(\"Recall: %3f\"%((TP)/(TP+FN)))\n",
    "print(\"Specificity: %3f\"%((TN)/(TN+FP)))\n",
    "precision = TP/(TP+FP)\n",
    "recall = (TP)/(TP+FN)\n",
    "F1Score = 2*recall*precision/(recall+precision)\n",
    "print(\"F1-Score: %3f\"%F1Score)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "myconda"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
