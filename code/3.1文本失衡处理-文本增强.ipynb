{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 文本增强"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synthetic comments（基于马尔科夫链随机生成）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data=pd.read_csv('C:/Users/25529/Downloads/jigsaw/train_data.csv', encoding='ANSI')\n",
    "toxic_data=train_data[train_data.toxic==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello, Nawlins Why do you keep fucking with people on Wikipedia by itself is BIASED!\n",
      "I've got a pussy ass pansy.\n",
      "Drive them into the fucking talk to prejudice people I know.\n",
      "I'm trying to tell you that have gotten raped by filthy pigs? which one is a moron in an Oxymoron Oxymoron is a greedy bastard who wants to talk to you or your allies were interested in your mother ass and pussy i want to know a father talks to a party, you always go around after other editors suck enormous amounts of vandalism attacks happening, and you know anything about a blog Why is it knowing you will not be given the fact they exist violate any copyright?\n",
      "I even suggested re-editing in the sinking ship USA...and it will be blocked and from Heron to Brunel, if thats racist, then sue me.\n",
      "Leave useful edit summaries are encouraged.\n",
      "== f.uck you == Brothers and sisters, In this article, but will he get so angry when I am a cunt.\n",
      "In case you haven't got any shame what is Modi doing about it?\n",
      "All good things, of course, on and was rude to me while using your spare hand to tickle my taint area gently?\n",
      "I went poking around and get the fuck you you little bitch?\n"
     ]
    }
   ],
   "source": [
    "import markovify as mk\n",
    "doc = train_data.loc[train_data.toxic==1, 'comment_text'].tolist()\n",
    "text_model = mk.Text(doc)\n",
    "for i in range(10):\n",
    "    print(text_model.make_sentence())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Synonym Replacement（同义词替换）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import random\n",
    "from nltk.corpus import wordnet\n",
    "def get_synonyms(word):\n",
    "    \n",
    "    synonyms = set()\n",
    "    \n",
    "    for syn in wordnet.synsets(word):\n",
    "        for l in syn.lemmas():\n",
    "            synonym = l.name().replace(\"_\", \" \").replace(\"-\", \" \").lower()\n",
    "            synonym = \"\".join([char for char in synonym if char in ' qwertyuiopasdfghjklzxcvbnm'])\n",
    "            synonyms.add(synonym) \n",
    "    if word in synonyms:\n",
    "        synonyms.remove(word)\n",
    "    \n",
    "    return list(synonyms)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = []\n",
    "for w in stopwords.words('english'):\n",
    "    stop_words.append(w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def synonym_replacement(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    \n",
    "    new_words = words.copy()\n",
    "    random_word_list = list(set([word for word in words if word not in stop_words]))\n",
    "    random.shuffle(random_word_list)\n",
    "    num_replaced = 0\n",
    "    \n",
    "    for random_word in random_word_list:\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        \n",
    "        if len(synonyms) >= 1:\n",
    "            synonym = random.choice(list(synonyms))\n",
    "            new_words = [synonym if word == random_word else word for word in new_words]\n",
    "            num_replaced += 1\n",
    "        \n",
    "        # n为随机替换单词数\n",
    "        if num_replaced >= n: \n",
    "            break\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "\n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, no. I ain't serving the war, I was helping people, This is fucked up.\n",
      " Example of Synonym Replacement: Hell, no. I ain't serving the war, I was helping people, This is fucked up.\n",
      " Example of Synonym Replacement: Hell, ordinal number I ain't serving the war, I was helping people, This is fucked up.\n",
      " Example of Synonym Replacement: Hell, no. i ain't serving the war, i was helping people, This is know up.\n"
     ]
    }
   ],
   "source": [
    "trial_sent =np.array(toxic_data.comment_text)[10]\n",
    "print(trial_sent)\n",
    "\n",
    "for n in range(3):\n",
    "    print(f\" Example of Synonym Replacement: {synonym_replacement(trial_sent,n)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25529\\AppData\\Local\\Temp\\ipykernel_21792\\2831844676.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  toxic_data['SR'][i:i+1]=synonym_replacement(np.array(toxic_data.comment_text)[i],3)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._set_values(indexer, value)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1056: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>SR</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15802</th>\n",
       "      <td>44cc46e639717428</td>\n",
       "      <td>LOL - you thought your sources - including the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL - you thought your germ - including the TB...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15803</th>\n",
       "      <td>44cc5d68e400d503</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>44d8a1aad4973721</td>\n",
       "      <td>Hot4\\nMy Ryan, do come back to me you gorgeous...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot4 My Ryan, do seed back to me you gorgeous ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>44ddd41a39cf01c5</td>\n",
       "      <td>How about you fuck off and don't stalk my edit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How about you shtup off and don't stem my edit...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>44e225b1139aa592</td>\n",
       "      <td>THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THERE HASNT exist whatever episode SINCE MARCH...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "15802  44cc46e639717428  LOL - you thought your sources - including the...   \n",
       "15803  44cc5d68e400d503  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...   \n",
       "15826  44d8a1aad4973721  Hot4\\nMy Ryan, do come back to me you gorgeous...   \n",
       "15833  44ddd41a39cf01c5  How about you fuck off and don't stalk my edit...   \n",
       "15839  44e225b1139aa592  THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "15802      1             0        1       0       0              0   \n",
       "15803      1             0        1       0       0              0   \n",
       "15826      1             0        0       0       0              0   \n",
       "15833      1             0        1       0       0              0   \n",
       "15839      1             0        1       0       0              0   \n",
       "\n",
       "                                                      SR  \n",
       "15802  LOL - you thought your germ - including the TB...  \n",
       "15803  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...  \n",
       "15826  Hot4 My Ryan, do seed back to me you gorgeous ...  \n",
       "15833  How about you shtup off and don't stem my edit...  \n",
       "15839  THERE HASNT exist whatever episode SINCE MARCH...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_data.insert(toxic_data.shape[1], 'SR', 0)\n",
    "for i in range(len(np.array(toxic_data.comment_text))):\n",
    "    toxic_data['SR'][i:i+1]=synonym_replacement(np.array(toxic_data.comment_text)[i],3)\n",
    "toxic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Deletion（随机删除）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def random_deletion(words, p):\n",
    "\n",
    "    words = words.split()\n",
    "    if len(words) == 1:\n",
    "        return words\n",
    "\n",
    "    #以概率p随机删除\n",
    "    new_words = []\n",
    "    for word in words:\n",
    "        r = random.uniform(0, 1)\n",
    "        if r > p:\n",
    "            new_words.append(word)\n",
    "            \n",
    "    #删除所有词语\n",
    "    if len(new_words) == 0:\n",
    "        rand_int = random.randint(0, len(words)-1)\n",
    "        return [words[rand_int]]\n",
    "\n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, no. ain't serving war, I This fucked\n",
      "no. war, I people, This is fucked\n",
      "ain't serving the was people, This is fucked\n"
     ]
    }
   ],
   "source": [
    "print(random_deletion(trial_sent,0.2))\n",
    "print(random_deletion(trial_sent,0.3))\n",
    "print(random_deletion(trial_sent,0.4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25529\\AppData\\Local\\Temp\\ipykernel_21792\\4266470768.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  toxic_data['RD'][i:i+1]=random_deletion(np.array(toxic_data.comment_text)[i],0.3)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._set_values(indexer, value)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1056: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>SR</th>\n",
       "      <th>RD</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15802</th>\n",
       "      <td>44cc46e639717428</td>\n",
       "      <td>LOL - you thought your sources - including the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL - you thought your germ - including the TB...</td>\n",
       "      <td>LOL your sources - the TBR more reliable News....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15803</th>\n",
       "      <td>44cc5d68e400d503</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...</td>\n",
       "      <td>HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>44d8a1aad4973721</td>\n",
       "      <td>Hot4\\nMy Ryan, do come back to me you gorgeous...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot4 My Ryan, do seed back to me you gorgeous ...</td>\n",
       "      <td>Hot4 come back me you gorgeous piece sex</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>44ddd41a39cf01c5</td>\n",
       "      <td>How about you fuck off and don't stalk my edit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How about you shtup off and don't stem my edit...</td>\n",
       "      <td>about you and don't stalk my 201.215.187.159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>44e225b1139aa592</td>\n",
       "      <td>THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THERE HASNT exist whatever episode SINCE MARCH...</td>\n",
       "      <td>THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "15802  44cc46e639717428  LOL - you thought your sources - including the...   \n",
       "15803  44cc5d68e400d503  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...   \n",
       "15826  44d8a1aad4973721  Hot4\\nMy Ryan, do come back to me you gorgeous...   \n",
       "15833  44ddd41a39cf01c5  How about you fuck off and don't stalk my edit...   \n",
       "15839  44e225b1139aa592  THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "15802      1             0        1       0       0              0   \n",
       "15803      1             0        1       0       0              0   \n",
       "15826      1             0        0       0       0              0   \n",
       "15833      1             0        1       0       0              0   \n",
       "15839      1             0        1       0       0              0   \n",
       "\n",
       "                                                      SR  \\\n",
       "15802  LOL - you thought your germ - including the TB...   \n",
       "15803  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...   \n",
       "15826  Hot4 My Ryan, do seed back to me you gorgeous ...   \n",
       "15833  How about you shtup off and don't stem my edit...   \n",
       "15839  THERE HASNT exist whatever episode SINCE MARCH...   \n",
       "\n",
       "                                                      RD  \n",
       "15802  LOL your sources - the TBR more reliable News....  \n",
       "15803       HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!  \n",
       "15826           Hot4 come back me you gorgeous piece sex  \n",
       "15833       about you and don't stalk my 201.215.187.159  \n",
       "15839     THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_data.insert(toxic_data.shape[1], 'RD', 0)\n",
    "for i in range(len(np.array(toxic_data.comment_text))):\n",
    "    toxic_data['RD'][i:i+1]=random_deletion(np.array(toxic_data.comment_text)[i],0.3)\n",
    "toxic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Swap（随机交换）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def swap_word(new_words):\n",
    "    \n",
    "    random_idx_1 = random.randint(0, len(new_words)-1)\n",
    "    random_idx_2 = random_idx_1\n",
    "    counter = 0\n",
    "    \n",
    "    while random_idx_2 == random_idx_1:\n",
    "        random_idx_2 = random.randint(0, len(new_words)-1)\n",
    "        counter += 1\n",
    "        \n",
    "        if counter > 3:\n",
    "            return new_words\n",
    "    \n",
    "    new_words[random_idx_1], new_words[random_idx_2] = new_words[random_idx_2], new_words[random_idx_1] \n",
    "    return new_words\n",
    "\n",
    "def random_swap(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    # n为随机交换单词数\n",
    "    for _ in range(n):\n",
    "        new_words = swap_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, no. is ain't serving the war, I was helping people, This I fucked up.\n",
      "Hell, no. I is serving the war, I was people, helping This ain't fucked up.\n",
      "I no. people, ain't serving the war, I was helping is This Hell, fucked up.\n"
     ]
    }
   ],
   "source": [
    "print(random_swap(trial_sent,1))\n",
    "print(random_swap(trial_sent,2))\n",
    "print(random_swap(trial_sent,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25529\\AppData\\Local\\Temp\\ipykernel_21792\\1945283172.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  toxic_data['RS'][i:i+1]=random_swap(np.array(toxic_data.comment_text)[i],3)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._set_values(indexer, value)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1056: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>SR</th>\n",
       "      <th>RD</th>\n",
       "      <th>RS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15802</th>\n",
       "      <td>44cc46e639717428</td>\n",
       "      <td>LOL - you thought your sources - including the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL - you thought your germ - including the TB...</td>\n",
       "      <td>LOL your sources - the TBR more reliable News....</td>\n",
       "      <td>LOL - News. thought your was - including the T...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15803</th>\n",
       "      <td>44cc5d68e400d503</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...</td>\n",
       "      <td>HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!</td>\n",
       "      <td>HAHA THIS WHATEVER WANT!!!!!! FUCKIN PAGE!!!!!...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>44d8a1aad4973721</td>\n",
       "      <td>Hot4\\nMy Ryan, do come back to me you gorgeous...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot4 My Ryan, do seed back to me you gorgeous ...</td>\n",
       "      <td>Hot4 come back me you gorgeous piece sex</td>\n",
       "      <td>Hot4 gorgeous Ryan, do back come to me you of ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>44ddd41a39cf01c5</td>\n",
       "      <td>How about you fuck off and don't stalk my edit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How about you shtup off and don't stem my edit...</td>\n",
       "      <td>about you and don't stalk my 201.215.187.159</td>\n",
       "      <td>off How you fuck about and don't edits? my sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>44e225b1139aa592</td>\n",
       "      <td>THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THERE HASNT exist whatever episode SINCE MARCH...</td>\n",
       "      <td>THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181</td>\n",
       "      <td>THERE BEEN SINCE ANY EPISODES HASNT MARCH DUMB...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "15802  44cc46e639717428  LOL - you thought your sources - including the...   \n",
       "15803  44cc5d68e400d503  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...   \n",
       "15826  44d8a1aad4973721  Hot4\\nMy Ryan, do come back to me you gorgeous...   \n",
       "15833  44ddd41a39cf01c5  How about you fuck off and don't stalk my edit...   \n",
       "15839  44e225b1139aa592  THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "15802      1             0        1       0       0              0   \n",
       "15803      1             0        1       0       0              0   \n",
       "15826      1             0        0       0       0              0   \n",
       "15833      1             0        1       0       0              0   \n",
       "15839      1             0        1       0       0              0   \n",
       "\n",
       "                                                      SR  \\\n",
       "15802  LOL - you thought your germ - including the TB...   \n",
       "15803  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...   \n",
       "15826  Hot4 My Ryan, do seed back to me you gorgeous ...   \n",
       "15833  How about you shtup off and don't stem my edit...   \n",
       "15839  THERE HASNT exist whatever episode SINCE MARCH...   \n",
       "\n",
       "                                                      RD  \\\n",
       "15802  LOL your sources - the TBR more reliable News....   \n",
       "15803       HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!   \n",
       "15826           Hot4 come back me you gorgeous piece sex   \n",
       "15833       about you and don't stalk my 201.215.187.159   \n",
       "15839     THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181   \n",
       "\n",
       "                                                      RS  \n",
       "15802  LOL - News. thought your was - including the T...  \n",
       "15803  HAHA THIS WHATEVER WANT!!!!!! FUCKIN PAGE!!!!!...  \n",
       "15826  Hot4 gorgeous Ryan, do back come to me you of ...  \n",
       "15833  off How you fuck about and don't edits? my sta...  \n",
       "15839  THERE BEEN SINCE ANY EPISODES HASNT MARCH DUMB...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_data.insert(toxic_data.shape[1], 'RS', 0)\n",
    "for i in range(len(np.array(toxic_data.comment_text))):\n",
    "    toxic_data['RS'][i:i+1]=random_swap(np.array(toxic_data.comment_text)[i],3)\n",
    "toxic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Insertion（随机插入）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_insertion(words, n):\n",
    "    \n",
    "    words = words.split()\n",
    "    new_words = words.copy()\n",
    "    \n",
    "    for _ in range(n):\n",
    "        add_word(new_words)\n",
    "        \n",
    "    sentence = ' '.join(new_words)\n",
    "    return sentence\n",
    "\n",
    "def add_word(new_words):\n",
    "    \n",
    "    synonyms = []\n",
    "    counter = 0\n",
    "    \n",
    "    while len(synonyms) < 1:\n",
    "        random_word = new_words[random.randint(0, len(new_words)-1)]\n",
    "        synonyms = get_synonyms(random_word)\n",
    "        counter += 1\n",
    "        if counter >= 10:\n",
    "            return\n",
    "        \n",
    "    random_synonym = synonyms[0]\n",
    "    random_idx = random.randint(0, len(new_words)-1)\n",
    "    new_words.insert(random_idx, random_synonym)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hell, cost no. I ain't serving the war, I was helping people, This is fucked up.\n",
      " Hell, no. I cost ain't serving the war, I was helping people, This is fucked up.\n",
      "Hell, cost no. I  ain't serving the war, I was helping people, This  is fucked up.\n"
     ]
    }
   ],
   "source": [
    "print(random_insertion(trial_sent,1))\n",
    "print(random_insertion(trial_sent,2))\n",
    "print(random_insertion(trial_sent,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\25529\\AppData\\Local\\Temp\\ipykernel_21792\\3123746593.py:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  toxic_data['RI'][i:i+1]=random_insertion(np.array(toxic_data.comment_text)[i],3)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1105: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  return self._set_values(indexer, value)\n",
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py:1056: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  cacher_needs_updating = self._check_is_chained_assignment_possible()\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>SR</th>\n",
       "      <th>RD</th>\n",
       "      <th>RS</th>\n",
       "      <th>RI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15802</th>\n",
       "      <td>44cc46e639717428</td>\n",
       "      <td>LOL - you thought your sources - including the...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>LOL - you thought your germ - including the TB...</td>\n",
       "      <td>LOL your sources - the TBR more reliable News....</td>\n",
       "      <td>LOL - News. thought your was - including the T...</td>\n",
       "      <td>LOL - you thought your sources - including the...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15803</th>\n",
       "      <td>44cc5d68e400d503</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...</td>\n",
       "      <td>HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!</td>\n",
       "      <td>HAHA THIS WHATEVER WANT!!!!!! FUCKIN PAGE!!!!!...</td>\n",
       "      <td>HAHA THIS cost IS MY FUCKIN PAGE!!!!!!!!! I  C...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15826</th>\n",
       "      <td>44d8a1aad4973721</td>\n",
       "      <td>Hot4\\nMy Ryan, do come back to me you gorgeous...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>Hot4 My Ryan, do seed back to me you gorgeous ...</td>\n",
       "      <td>Hot4 come back me you gorgeous piece sex</td>\n",
       "      <td>Hot4 gorgeous Ryan, do back come to me you of ...</td>\n",
       "      <td>Hot4 make My Ryan, do come back to make out me...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15833</th>\n",
       "      <td>44ddd41a39cf01c5</td>\n",
       "      <td>How about you fuck off and don't stalk my edit...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>How about you shtup off and don't stem my edit...</td>\n",
       "      <td>about you and don't stalk my 201.215.187.159</td>\n",
       "      <td>off How you fuck about and don't edits? my sta...</td>\n",
       "      <td>dispatch How about you fuck off still hunt and...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15839</th>\n",
       "      <td>44e225b1139aa592</td>\n",
       "      <td>THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>THERE HASNT exist whatever episode SINCE MARCH...</td>\n",
       "      <td>THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181</td>\n",
       "      <td>THERE BEEN SINCE ANY EPISODES HASNT MARCH DUMB...</td>\n",
       "      <td>any THERE HASNT BEEN ANY any EPISODES SINCE MA...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     id                                       comment_text  \\\n",
       "15802  44cc46e639717428  LOL - you thought your sources - including the...   \n",
       "15803  44cc5d68e400d503  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! I CAN DO ...   \n",
       "15826  44d8a1aad4973721  Hot4\\nMy Ryan, do come back to me you gorgeous...   \n",
       "15833  44ddd41a39cf01c5  How about you fuck off and don't stalk my edit...   \n",
       "15839  44e225b1139aa592  THERE HASNT BEEN ANY EPISODES SINCE MARCH 7, D...   \n",
       "\n",
       "       toxic  severe_toxic  obscene  threat  insult  identity_hate  \\\n",
       "15802      1             0        1       0       0              0   \n",
       "15803      1             0        1       0       0              0   \n",
       "15826      1             0        0       0       0              0   \n",
       "15833      1             0        1       0       0              0   \n",
       "15839      1             0        1       0       0              0   \n",
       "\n",
       "                                                      SR  \\\n",
       "15802  LOL - you thought your germ - including the TB...   \n",
       "15803  HAHA THIS IS MY FUCKIN PAGE!!!!!!!!! ace CAN b...   \n",
       "15826  Hot4 My Ryan, do seed back to me you gorgeous ...   \n",
       "15833  How about you shtup off and don't stem my edit...   \n",
       "15839  THERE HASNT exist whatever episode SINCE MARCH...   \n",
       "\n",
       "                                                      RD  \\\n",
       "15802  LOL your sources - the TBR more reliable News....   \n",
       "15803       HAHA THIS IS FUCKIN DO WHATEVER I WANT!!!!!!   \n",
       "15826           Hot4 come back me you gorgeous piece sex   \n",
       "15833       about you and don't stalk my 201.215.187.159   \n",
       "15839     THERE HASNT BEEN SINCE MARCH 7, 50.180.208.181   \n",
       "\n",
       "                                                      RS  \\\n",
       "15802  LOL - News. thought your was - including the T...   \n",
       "15803  HAHA THIS WHATEVER WANT!!!!!! FUCKIN PAGE!!!!!...   \n",
       "15826  Hot4 gorgeous Ryan, do back come to me you of ...   \n",
       "15833  off How you fuck about and don't edits? my sta...   \n",
       "15839  THERE BEEN SINCE ANY EPISODES HASNT MARCH DUMB...   \n",
       "\n",
       "                                                      RI  \n",
       "15802  LOL - you thought your sources - including the...  \n",
       "15803  HAHA THIS cost IS MY FUCKIN PAGE!!!!!!!!! I  C...  \n",
       "15826  Hot4 make My Ryan, do come back to make out me...  \n",
       "15833  dispatch How about you fuck off still hunt and...  \n",
       "15839  any THERE HASNT BEEN ANY any EPISODES SINCE MA...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "toxic_data.insert(toxic_data.shape[1], 'RI', 0)\n",
    "for i in range(len(np.array(toxic_data.comment_text))):\n",
    "    toxic_data['RI'][i:i+1]=random_insertion(np.array(toxic_data.comment_text)[i],3)\n",
    "toxic_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 扩充样本量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if there is a chromosone then e=what is it?Sma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hollywood Undead \\n\\nI have collected articles...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\n Rollback \\n\\nI've enabled rollback on yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Another backlog. Thanks. (Trouble?/My Work)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" - unsigned\\n\\nWe do include it. This article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213371</th>\n",
       "      <td>:Jerome, I see you never got around to this…! ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213372</th>\n",
       "      <td>==Lucky bastard== \\n http://wikimediafoundatio...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213373</th>\n",
       "      <td>==shame on you all!!!== \\n\\n You want to speak...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213374</th>\n",
       "      <td>MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213375</th>\n",
       "      <td>\" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>213376 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "0       if there is a chromosone then e=what is it?Sma...      0\n",
       "1       Hollywood Undead \\n\\nI have collected articles...      0\n",
       "2       \"\\n\\n Rollback \\n\\nI've enabled rollback on yo...      0\n",
       "3             Another backlog. Thanks. (Trouble?/My Work)      0\n",
       "4       \" - unsigned\\n\\nWe do include it. This article...      0\n",
       "...                                                   ...    ...\n",
       "213371  :Jerome, I see you never got around to this…! ...      0\n",
       "213372  ==Lucky bastard== \\n http://wikimediafoundatio...      0\n",
       "213373  ==shame on you all!!!== \\n\\n You want to speak...      0\n",
       "213374  MEL GIBSON IS A NAZI BITCH WHO MAKES SHITTY MO...      1\n",
       "213375  \" \\n\\n == Unicorn lair discovery == \\n\\n Suppo...      0\n",
       "\n",
       "[213376 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data[['comment_text', 'toxic']]\n",
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>if there is a chromosone then e=what is it?Sma...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hollywood Undead \\n\\nI have collected articles...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"\\n\\n Rollback \\n\\nI've enabled rollback on yo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Another backlog. Thanks. (Trouble?/My Work)</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>\" - unsigned\\n\\nWe do include it. This article...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213263</th>\n",
       "      <td>am sorry for being a dickhead! I cannae help i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213275</th>\n",
       "      <td>NIGEL cost adenine IS A CRAZY cost IDIOT!!!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213343</th>\n",
       "      <td>==Fourth Baldrick possibly being cleverer than...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213360</th>\n",
       "      <td>\" == IRAN == That’s right, Iran. It was our dr...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213374</th>\n",
       "      <td>gain MEL GIBSON who IS A NAZI BITCH WHO MAKES ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>288812 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             comment_text  toxic\n",
       "0       if there is a chromosone then e=what is it?Sma...      0\n",
       "1       Hollywood Undead \\n\\nI have collected articles...      0\n",
       "2       \"\\n\\n Rollback \\n\\nI've enabled rollback on yo...      0\n",
       "3             Another backlog. Thanks. (Trouble?/My Work)      0\n",
       "4       \" - unsigned\\n\\nWe do include it. This article...      0\n",
       "...                                                   ...    ...\n",
       "213263  am sorry for being a dickhead! I cannae help i...      1\n",
       "213275        NIGEL cost adenine IS A CRAZY cost IDIOT!!!      1\n",
       "213343  ==Fourth Baldrick possibly being cleverer than...      1\n",
       "213360  \" == IRAN == That’s right, Iran. It was our dr...      1\n",
       "213374  gain MEL GIBSON who IS A NAZI BITCH WHO MAKES ...      1\n",
       "\n",
       "[288812 rows x 2 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data = train_data.append(toxic_data[['SR', 'toxic']].rename(columns={'SR': 'comment_text'}))\n",
    "train_data = train_data.append(toxic_data[['RD', 'toxic']].rename(columns={'RD': 'comment_text'}))\n",
    "train_data = train_data.append(toxic_data[['RS', 'toxic']].rename(columns={'RS': 'comment_text'}))\n",
    "train_data = train_data.append(toxic_data[['RI', 'toxic']].rename(columns={'RI': 'comment_text'}))\n",
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### BERT评测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一句话: [CLS] if there is a chromosone then e=what is it?Small Text [SEP]\n",
      "tokenized的第一句话: ['[CLS]', 'if', 'there', 'is', 'a', 'ch', '##rom', '##oso', '##ne', 'then', 'e', '=', 'what', 'is', 'it', '?', 'small', 'text', '[SEP]']\n"
     ]
    }
   ],
   "source": [
    "data = train_data\n",
    "#提取语句并处理\n",
    "sentencses=['[CLS] ' + sent + ' [SEP]' for sent in data.comment_text.values]\n",
    "labels=data.toxic.values\n",
    "print(\"第一句话:\",sentencses[0])\n",
    "tokenizer=BertTokenizer.from_pretrained(bert_pre_tokenizer,do_lower_case=True)\n",
    "tokenized_sents=[tokenizer.tokenize(sent) for sent in sentencses]\n",
    "print(\"tokenized的第一句话:\",tokenized_sents[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "转化后的第一个句子: [101, 2065, 2045, 2003, 1037, 10381, 21716, 19137, 2638, 2059, 1041, 1027, 2054, 2003, 2009, 1029, 2235, 3793, 102]\n"
     ]
    }
   ],
   "source": [
    "#定义句子最大长度\n",
    "MAX_LEN=96\n",
    "#将分割后的句子转化成数字  word-->idx\n",
    "input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_sents]\n",
    "print(\"转化后的第一个句子:\",input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Padding 第一个句子: [  101  2065  2045  2003  1037 10381 21716 19137  2638  2059  1041  1027\n",
      "  2054  2003  2009  1029  2235  3793   102     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n"
     ]
    }
   ],
   "source": [
    "#PADDING\n",
    "import keras\n",
    "input_ids=keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(\"Padding 第一个句子:\",input_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一个attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#mask\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "print(\"第一个attention mask:\",attention_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集的一个inputs [  101  1045  4384  3158  9305 22556  2006 13292  1012  2322  2008  2081\n",
      "  2033  2514  2009  2052  5258  2153  1012   102     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "训练集的一个mask [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#划分训练集、验证集\n",
    "train_inputs, validation_inputs, train_labels, validation_labels = train_test_split(input_ids, labels,\n",
    "                                                            random_state=2018, test_size=0.1)\n",
    "train_masks, validation_masks, _, _ = train_test_split(attention_masks, input_ids,\n",
    "                                             random_state=2018, test_size=0.1)\n",
    "print(\"训练集的一个inputs\",train_inputs[0])\n",
    "print(\"训练集的一个mask\",train_masks[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#将训练集、验证集转化成tensor\n",
    "train_inputs = torch.tensor(train_inputs)\n",
    "validation_inputs = torch.tensor(validation_inputs)\n",
    "train_labels = torch.tensor(train_labels)\n",
    "validation_labels = torch.tensor(validation_labels)\n",
    "train_masks = torch.tensor(train_masks)\n",
    "validation_masks = torch.tensor(validation_masks)\n",
    "\n",
    "#生成dataloader\n",
    "batch_size = 16\n",
    "train_data = Data.TensorDataset(train_inputs, train_masks, train_labels)\n",
    "train_sampler = Data.RandomSampler(train_data)\n",
    "train_dataloader = Data.DataLoader(train_data, sampler=train_sampler, batch_size=batch_size)\n",
    "validation_data = Data.TensorDataset(validation_inputs, validation_masks, validation_labels)\n",
    "validation_sampler = Data.SequentialSampler(validation_data)\n",
    "validation_dataloader = Data.DataLoader(validation_data, sampler=validation_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at C:/Users/25529/Downloads/bert-base-uncased/pytorch_model.bin were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at C:/Users/25529/Downloads/bert-base-uncased/pytorch_model.bin and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BertForSequenceClassification(\n",
      "  (bert): BertModel(\n",
      "    (embeddings): BertEmbeddings(\n",
      "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
      "      (position_embeddings): Embedding(512, 768)\n",
      "      (token_type_embeddings): Embedding(2, 768)\n",
      "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "      (dropout): Dropout(p=0.1, inplace=False)\n",
      "    )\n",
      "    (encoder): BertEncoder(\n",
      "      (layer): ModuleList(\n",
      "        (0): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (1): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (2): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (3): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (4): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (5): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (6): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (7): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (8): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (9): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (10): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "        (11): BertLayer(\n",
      "          (attention): BertAttention(\n",
      "            (self): BertSelfAttention(\n",
      "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "            (output): BertSelfOutput(\n",
      "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "              (dropout): Dropout(p=0.1, inplace=False)\n",
      "            )\n",
      "          )\n",
      "          (intermediate): BertIntermediate(\n",
      "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
      "          )\n",
      "          (output): BertOutput(\n",
      "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
      "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
      "            (dropout): Dropout(p=0.1, inplace=False)\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (pooler): BertPooler(\n",
      "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
      "      (activation): Tanh()\n",
      "    )\n",
      "  )\n",
      "  (dropout): Dropout(p=0.1, inplace=False)\n",
      "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "modelConfig = BertConfig.from_pretrained(bert_config)\n",
    "model = BertForSequenceClassification.from_pretrained(bert_pre_model, config=modelConfig)\n",
    "print(model.cuda())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "t_total value of -1 results in schedule not being applied\n"
     ]
    }
   ],
   "source": [
    "param_optimizer = list(model.named_parameters())\n",
    "no_decay = ['bias', 'gamma', 'beta']\n",
    "optimizer_grouped_parameters = [\n",
    "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.01},\n",
    "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
    "     'weight_decay_rate': 0.0}\n",
    "]\n",
    "\n",
    "optimizer = BertAdam(optimizer_grouped_parameters,\n",
    "                     lr=2e-5,\n",
    "                     warmup=.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#定义一个计算准确率的函数\n",
    "def flat_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\pytorch_pretrained_bert\\optimization.py:275: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at  C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1055.)\n",
      "  next_m.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.13139060938444605\n",
      "Validation Accuracy: 0.9598560354374308\n",
      "Train loss: 0.06520119497842784\n",
      "Validation Accuracy: 0.9658776301218162\n",
      "Train loss: 0.036378856597790436\n",
      "Validation Accuracy: 0.9681616832779624\n"
     ]
    }
   ],
   "source": [
    "#训练开始\n",
    "train_loss_set = []#可以将loss加入到列表中，后期画图使用\n",
    "epochs = 3\n",
    "os.environ['CUDA_DEVICE_ORDER'] = 'PCI_BUS_ID'\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = '1'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")#判断CUDA是否能使用，不可以就使用CPU\n",
    "for _ in range(epochs):\n",
    "    #训练开始\n",
    "    model.train()\n",
    "    tr_loss = 0\n",
    "    nb_tr_examples, nb_tr_steps = 0, 0\n",
    "    for step, batch in enumerate(train_dataloader):\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        optimizer.zero_grad()\n",
    "        #取第一个位置，BertForSequenceClassification第一个位置是Loss，第二个位置是[CLS]的logits\n",
    "        loss = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)[0]\n",
    "        train_loss_set.append(loss.item())\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        tr_loss += loss.item()\n",
    "        nb_tr_examples += b_input_ids.size(0)\n",
    "        nb_tr_steps += 1\n",
    "    print(\"Train loss: {}\".format(tr_loss / nb_tr_steps))\n",
    "    #模型评估\n",
    "    model.eval()\n",
    "    eval_loss, eval_accuracy = 0, 0\n",
    "    nb_eval_steps, nb_eval_examples = 0, 0\n",
    "    for batch in validation_dataloader:\n",
    "        batch = tuple(t.to(device) for t in batch)\n",
    "        b_input_ids, b_input_mask, b_labels = batch\n",
    "        with torch.no_grad():\n",
    "            logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "        logits = logits.detach().cpu().numpy()\n",
    "        label_ids = b_labels.to('cpu').numpy()\n",
    "        tmp_eval_accuracy = flat_accuracy(logits, label_ids)\n",
    "        eval_accuracy += tmp_eval_accuracy\n",
    "        nb_eval_steps += 1\n",
    "    print(\"Validation Accuracy: {}\".format(eval_accuracy / nb_eval_steps))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 保存模型参数\n",
    "def save_model(the_model, PATH):\n",
    "    torch.save(the_model, PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(model, 'C:/Users/25529/Downloads/jigsaw_BERT_textAugmentation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Hey... what is it..\\n@ | talk .\\nWhat is it......</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Bye! \\n\\nDon't look, come or think of comming ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>You are gay or antisemmitian? \\n\\nArchangel WH...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                        comment_text  toxic\n",
       "0       COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK      1\n",
       "1  Hey... what is it..\\n@ | talk .\\nWhat is it......      1\n",
       "2  Bye! \\n\\nDon't look, come or think of comming ...      1\n",
       "3  You are gay or antisemmitian? \\n\\nArchangel WH...      1\n",
       "4           FUCK YOUR FILTHY MOTHER IN THE ASS, DRY!      1"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "test_data = pd.read_csv('C:/Users/25529/Downloads/jigsaw/test_data.csv', encoding='ISO-8859-1')\n",
    "test_data = test_data[['comment_text', 'toxic']]\n",
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "第一句话: [CLS] COCKSUCKER BEFORE YOU PISS AROUND ON MY WORK [SEP]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\25529\\anaconda3\\lib\\site-packages\\transformers\\tokenization_utils_base.py:1628: FutureWarning: Calling BertTokenizer.from_pretrained() with the path to a single file or url is deprecated and won't be possible anymore in v5. Use a model identifier or the path to a directory instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tokenized的第一句话: ['[CLS]', 'cock', '##su', '##cker', 'before', 'you', 'piss', 'around', 'on', 'my', 'work', '[SEP]']\n",
      "转化后的第一个句子: [101, 10338, 6342, 9102, 2077, 2017, 18138, 2105, 2006, 2026, 2147, 102]\n",
      "Padding 第一个句子: [  101 10338  6342  9102  2077  2017 18138  2105  2006  2026  2147   102\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0]\n",
      "第一个attention mask: [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "#提取语句并处理\n",
    "sentencses=['[CLS] ' + sent + ' [SEP]' for sent in test_data.comment_text.values]\n",
    "labels=test_data.toxic.values\n",
    "print(\"第一句话:\",sentencses[0])\n",
    "tokenizer=BertTokenizer.from_pretrained(bert_pre_tokenizer,do_lower_case=True)\n",
    "tokenized_sents=[tokenizer.tokenize(sent) for sent in sentencses]\n",
    "print(\"tokenized的第一句话:\",tokenized_sents[0])\n",
    "#将分割后的句子转化成数字  word-->idx\n",
    "input_ids=[tokenizer.convert_tokens_to_ids(sent) for sent in tokenized_sents]\n",
    "print(\"转化后的第一个句子:\",input_ids[0])\n",
    "#PADDING\n",
    "import keras\n",
    "input_ids=keras.preprocessing.sequence.pad_sequences(input_ids, maxlen=MAX_LEN, dtype=\"long\", truncating=\"post\", padding=\"post\")\n",
    "print(\"Padding 第一个句子:\",input_ids[0])\n",
    "#mask\n",
    "attention_masks = []\n",
    "for seq in input_ids:\n",
    "  seq_mask = [float(i>0) for i in seq]\n",
    "  attention_masks.append(seq_mask)\n",
    "print(\"第一个attention mask:\",attention_masks[0])\n",
    "#转化成tensor\n",
    "test_inputs = torch.tensor(input_ids)\n",
    "test_labels = torch.tensor(labels)\n",
    "test_masks = torch.tensor(attention_masks)\n",
    "#生成dataloader\n",
    "batch_size = 64\n",
    "test_data = Data.TensorDataset(test_inputs, test_masks, test_labels)\n",
    "test_sampler = Data.RandomSampler(test_data)\n",
    "test_dataloader = Data.DataLoader(test_data, sampler=test_sampler, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#计算recall\n",
    "def flat_recall(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return recall_score(labels_flat, pred_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.918996159397876\n",
      "Test Recall: 0.7057049344456292\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "test_loss, test_accuracy, test_recall = 0, 0, 0\n",
    "nb_test_steps, nb_test_examples = 0, 0\n",
    "for batch in test_dataloader:\n",
    "    batch = tuple(t.to(device) for t in batch)\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    with torch.no_grad():\n",
    "        logits = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)[0]\n",
    "    logits = logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    tmp_test_accuracy = flat_accuracy(logits, label_ids)\n",
    "    tmp_test_recall = flat_recall(logits, label_ids)\n",
    "    test_accuracy += tmp_test_accuracy\n",
    "    test_recall += tmp_test_recall\n",
    "    nb_test_steps += 1\n",
    "print(\"Test Accuracy: {}\".format(test_accuracy / nb_test_steps))\n",
    "print(\"Test Recall: {}\".format(test_recall / nb_test_steps))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
